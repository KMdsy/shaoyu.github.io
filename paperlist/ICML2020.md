# ICML 2020 (2020.07.12)
[Link](https://icml.cc/Conferences/2020/Schedule?type=Poster)

[Schedule](https://icml.cc/Conferences/2020/Schedule)

## Anomaly

+ Interpretable, Multidimensional, Multimodal Anomaly Detection with Negative Sampling for Detection of Device Failure

  John Sipple

## Sequence

+ Population-Based Black-Box Optimization for Biological Sequence Design

  Christof Angermueller · David Belanger · Andreea Gane · Zelda Mariet · David Dohan · Kevin Murphy · Lucy Colwell · D. Sculley

+ A Chance-Constrained Generative Framework for Sequence Optimization

  Xianggen Liu · Jian Peng · Qiang Liu · Sen Song

+ An EM Approach to Non-autoregressive Conditional Sequence Generation

  Zhiqing Sun · Yiming Yang

+ CAUSE: Learning Granger Causality from Event Sequences using Attribution Methods

  Wei Zhang · Thomas Panum · Somesh Jha · Prasad Chalasani · David Page

+ Imputer: Sequence Modelling via Imputation and Dynamic Programming

  William Chan · Chitwan Saharia · Geoffrey Hinton · Mohammad Norouzi · Navdeep Jaitly

+ Incremental Sampling Without Replacement for Sequence Models

  Kensen Shi · David Bieber · Charles Sutton

+ Sequence Generation with Mixed Representations

  Lijun Wu · Shufang Xie · Yingce Xia · Yang Fan · Jian-Huang Lai · Tao Qin · Tie-Yan Liu

## Time Series

+ Learning From Irregularly-Sampled Time Series: A Missing Data Perspective

  Steven Cheng-Xian Li · Benjamin M Marlin

+ Set Functions for Time Series

  Max Horn · Michael Moor · Christian Bock · Bastian Rieck · Karsten Borgwardt
  
  *所解决的问题：针对非规则采样的时间序列的分类问题，文中的大意是还可以进行其他下游任务，而非仅有分类（更换相应的loss函数即可）。Dataset: MIMIC-III Mortality Prediction, Physionet 2012 Mortality Prediction Challenge, Physionet 2019 Sepsis Early Prediction Challenge*
  
  
  *一个非常规的做法，其理论依据是differentiable set function learning, 具备可解释性是其优点，
  在具体做法方面，pipeline如下：**多维时间序列** -> **encoding** (不使用类似seq2seq的深度学习方法，而是使用一种类似频率域分解的固定encodeing方式) -> **Embedding+Aggregation+Attention** (这一步使用深度神经网络，attention为scaled dot-product attention with multiple heads) -> **classification** (神经网络)。*
  
  *在性能方面，所提出的方法分类精度并不能达到state-of-the-art，只是和baseline有竞争性，而在可解释性、内存占用、运行时间上有优异的性能。
  BTW, baseline: GRU-D, GRU-SIMPLE, IP-NETS, PHASED-LSTM, TRANSFORMER, SEFT-ATTN*

+ Spectral Subsampling MCMC for Stationary Time Series

  Robert Salomone · Matias Quiroz · Robert kohn · Mattias Villani · Minh-Ngoc Tran
  
+ Temporal Logic Point Processes

  Shuang Li · Lu Wang · Ruizhi Zhang · xiaofu Chang · Xuqin Liu · Yao Xie · Yuan Qi · Le Song

+ Gradient Temporal-Difference Learning with Regularized Corrections

  Sina Ghiassian · Andrew Patterson · Shivam Garg · Dhawal Gupta · Adam White · Martha White
  
  
## missing value

+ Learning from Irregularly-Sampled Time Series: A Missing Data Perspective

  Steven Cheng-Xian Li · Benjamin M Marlin


+ Missing Data Imputation using Optimal Transport

  Boris Muzellec · Julie Josse · Claire Boyer · Marco Cuturi

## Recurrent

+ A general recurrent state space framework for modeling neural dynamics during decision-making

  David Zoltowski · Jonathan Pillow · Scott Linderman

+ Improving the Gating Mechanism of Recurrent Neural Networks

  Albert Gu · Caglar Gulcehre · Thomas Paine · Matthew Hoffman · Razvan Pascanu

+ Approximating Stacked and Bidirectional Recurrent Architectures with the Delayed Recurrent Neural Network

  Javier Turek · Shailee Jain · Vy Vo · Mihai Capotă · Alexander Huth · Theodore Willke

+ Transformation of ReLU-based recurrent neural networks from discrete-time to continuous-time

  Zahra Monfared · Daniel Durstewitz

+ Learning to Combine Top-Down and Bottom-Up Signals in Recurrent Neural Networks with Attention over Modules

  Sarthak Mittal · Alex Lamb · Anirudh Goyal · Vikram Voleti · Murray Shanahan · Guillaume Lajoie · Michael Mozer · Yoshua Bengio

+ VideoOneNet: Bidirectional Convolutional Recurrent OneNet with Trainable Data Steps for Video Processing

  Zoltán Milacski · Barnabás Póczos · Andras Lorincz

+ Frequentist Uncertainty in Recurrent Neural Networks via Blockwise Influence Functions

  Ahmed Alaa · Mihaela van der Schaar

## Interpretable

+ Multiresolution Tensor Learning for Efficient and Interpretable Spatial Analysis

  Jung Yeon Park · Kenneth Carr · Stephan Zheng · Yisong Yue · Rose Yu

+ Interpretable Off-Policy Evaluation in Reinforcement Learning by Highlighting Influential Transitions

  Omer Gottesman · Joseph Futoma · Yao Liu · Sonali Parbhoo · Leo Celi · Emma Brunskill · Finale Doshi-Velez

+ Dispersed EM-VAEs for Interpretable Text Generation

  Wenxian Shi · Hao Zhou · Ning Miao · Lei Li

+ Interpretable, Multidimensional, Multimodal Anomaly Detection with Negative Sampling for Detection of Device Failure

  John Sipple

## Autoencoder

+ Encoding Musical Style with Transformer Autoencoders

  Kristy Choi · Curtis Hawthorne · Ian Simon · Monica Dinculescu · Jesse Engel

+ Forecasting sequential data using Consistent Koopman Autoencoders

  Omri Azencot · N. Benjamin Erichson · Vanessa Lin · Michael Mahoney

+ Latent Bernoulli Autoencoder

  Jiri Fajtl · Vasileios Argyriou · Dorothy Monekosso · Paolo Remagnino

+ Perceptual Generative Autoencoders

  Zijun Zhang · Ruixiang ZHANG · Zongpeng Li · Yoshua Bengio · Liam Paull

+ Rate-distortion optimization guided autoencoder for isometric embedding in Euclidean latent space

  Keizo Kato · Jing Zhou · Tomotake Sasaki · Akira Nakagawa

+ Educating Text Autoencoders: Latent Representation Guidance via Denoising

  Tianxiao Shen · Jonas Mueller · Regina Barzilay · Tommi Jaakkola

+ Topological Autoencoders

  Michael Moor · Max Horn · Bastian Rieck · Karsten Borgwardt

+ ControlVAE: Controllable Variational Autoencoder

  Huajie Shao · Shuochao Yao · Dachun Sun · Aston Zhang · Shengzhong Liu · Dongxin Liu · Jun Wang · Tarek Abdelzaher

+ Learning Autoencoders with Relational Regularization

  Hongteng Xu · Dixin Luo · Ricardo Henao · Svati Shah · Lawrence Carin

## LSTM

+ Do RNN and LSTM have Long Memory?

  Jingyu Zhao · Feiqing Huang · Jia Lv · Yanjie Duan · Zhen Qin · Guodong Li · Guangjian Tian
  
  
## Data augmentation

+ On the Generalization Effects of Linear Transformations in Data Augmentation
  
  Sen Wu · Hongyang Zhang · Gregory Valiant · Christopher Re
  
+ Improving Molecular Design by Stochastic Iterative Target Augmentation
  
  Kevin Yang · Wengong Jin · Kyle Swanson · Regina Barzilay · Tommi Jaakkola


+  VFlow: More Expressive Generative Flows with Variational Data Augmentation
  
  Jianfei Chen · Cheng Lu · Biqi Chenli · Jun Zhu · Tian Tian

+ Self-supervised Label Augmentation via Input Transformations

  Hankook Lee · Sung Ju Hwang · Jinwoo Shin
  
  一种在全监督的设定下，做数据增强的方案。所提出的方法：
  
  1. 区别于传统的self-supervised learning task（multi-task leanring: 设定两个任务，一个原始任务一个附加任务，使用两个子网络进行实现，loss函数为两个任务的和），
  本文提出的方法直接在同一个网络中同时进行原始任务+附加任务。
  
  即：对于一个图片分类任务，传统方法：子网络1用于分类，子网络2用于区分数据增强方法。本文方法：训练一个$N+M$分类的网络，前者为数据类别数，后者为增强方法数。然后对网络的输出进行融合，学习一个条件概率分布，然后将该概率的loss加入到总的loss中。
  
  

+ Distribution Augmentation for Generative Modeling
  
  Heewoo Jun · Rewon Child · Mark Chen · John Schulman · Aditya Ramesh · Alec Radford ·
Ilya Sutskever
