# ICML 2020 (2020.07.12)
[Link](https://icml.cc/Conferences/2020/Schedule?type=Poster)

[Schedule](https://icml.cc/Conferences/2020/Schedule)

## Anomaly

+ **[大概看了看，不太推荐]** Interpretable, Multidimensional, Multimodal Anomaly Detection with Negative Sampling for Detection of Device Failure

  John Sipple
  
  CONTRIBUTIONS: 
  
  + a scalable approach to detecting multidimensional outliers, robust under multimodal conditions,
  + an alternative to one-class anomaly detection using negative sampling, and
  + a novel application of Integrated Gradients for anomaly interpretation.

+ Robust Outlier Arm Identification

  Yinglun Zhu · Sumeet Katariya · Robert Nowak
  
## Sequence

+ Population-Based Black-Box Optimization for Biological Sequence Design

  Christof Angermueller · David Belanger · Andreea Gane · Zelda Mariet · David Dohan · Kevin Murphy · Lucy Colwell · D. Sculley

+ A Chance-Constrained Generative Framework for Sequence Optimization

  Xianggen Liu · Jian Peng · Qiang Liu · Sen Song

+ An EM Approach to Non-autoregressive Conditional Sequence Generation

  Zhiqing Sun · Yiming Yang

+ CAUSE: Learning Granger Causality from Event Sequences using Attribution Methods

  Wei Zhang · Thomas Panum · Somesh Jha · Prasad Chalasani · David Page

+ Imputer: Sequence Modelling via Imputation and Dynamic Programming

  William Chan · Chitwan Saharia · Geoffrey Hinton · Mohammad Norouzi · Navdeep Jaitly

+ Incremental Sampling Without Replacement for Sequence Models

  Kensen Shi · David Bieber · Charles Sutton

+ Sequence Generation with Mixed Representations

  Lijun Wu · Shufang Xie · Yingce Xia · Yang Fan · Jian-Huang Lai · Tao Qin · Tie-Yan Liu

## Time Series

+ **[已读]** Learning From Irregularly-Sampled Time Series: A Missing Data Perspective

  Steven Cheng-Xian Li · Benjamin M Marlin

+ **[已读]** Set Functions for Time Series

  Max Horn · Michael Moor · Christian Bock · Bastian Rieck · Karsten Borgwardt
  
  *所解决的问题：针对非规则采样的时间序列的分类问题，文中的大意是还可以进行其他下游任务，而非仅有分类（更换相应的loss函数即可）。Dataset: MIMIC-III Mortality Prediction, Physionet 2012 Mortality Prediction Challenge, Physionet 2019 Sepsis Early Prediction Challenge*
  
  
  *一个非常规的做法，其理论依据是differentiable set function learning, 具备可解释性是其优点，
  在具体做法方面，pipeline如下：**多维时间序列** -> **encoding** (不使用类似seq2seq的深度学习方法，而是使用一种类似频率域分解的固定encodeing方式) -> **Embedding+Aggregation+Attention** (这一步使用深度神经网络，attention为scaled dot-product attention with multiple heads) -> **classification** (神经网络)。*
  
  *在性能方面，所提出的方法分类精度并不能达到state-of-the-art，只是和baseline有竞争性，而在可解释性、内存占用、运行时间上有优异的性能。
  BTW, baseline: GRU-D, GRU-SIMPLE, IP-NETS, PHASED-LSTM, TRANSFORMER, SEFT-ATTN*

+ Spectral Subsampling MCMC for Stationary Time Series

  Robert Salomone · Matias Quiroz · Robert kohn · Mattias Villani · Minh-Ngoc Tran
  
+ Temporal Logic Point Processes

  Shuang Li · Lu Wang · Ruizhi Zhang · xiaofu Chang · Xuqin Liu · Yao Xie · Yuan Qi · Le Song

+ Gradient Temporal-Difference Learning with Regularized Corrections

  Sina Ghiassian · Andrew Patterson · Shivam Garg · Dhawal Gupta · Adam White · Martha White
  
+ Time Series Deconfounder: Estimating Treatment Effects over Time in the Presence of Hidden Confounders

  Ioana Bica · Ahmed Alaa · Mihaela van der Schaar
  
  
## missing value

+ **[已读]** Learning from Irregularly-Sampled Time Series: A Missing Data Perspective

  Steven Cheng-Xian Li · Benjamin M Marlin
  
  *解决的问题:* 不规则采样、有缺失时间序列建模问题、上述时间序列的分类问题。
  
  *创新与独特:* 提出P-VAE与P-GAN（Partial – VAE/GAN）。 将缺失值补全问题转化为一个插值问题
  
  *采用的方法:* 
    + 在P-VAE中，将可观测到的点集合T视为从某个分布上采样的随机变量，将VAE中的关于x的概率分布转化为x.t的联合分布。并仅监督可观测部分的重建误差与生成误差
    + 在P-GAN中，分别向判别中送入（x, t, z）对，以使得有缺失的真实数据与生成的完整数据有相似的隐空间表达、相似的可观测数据。
  *为何选择/如何应用:* 本文所提出的P-VAE中的理论推导部分值的后续工作借鉴
  
  *数据集*
  + 图像数据集：MNIST、CelebA
  + 时间序列数据集：MIMIC-II（分类任务）



+ Missing Data Imputation using Optimal Transport

  Boris Muzellec · Julie Josse · Claire Boyer · Marco Cuturi
  
+ Learning with Missing Values [workshop]

  Julie Josse · Jes Frellsen · Pierre-Alexandre Mattei · Gael Varoquaux
  
+ Full Law Identification in Graphical Models of Missing Data: Completeness Results

  Razieh Nabi · Rohit Bhattacharya · Ilya Shpitser

## Recurrent

+ A general recurrent state space framework for modeling neural dynamics during decision-making

  David Zoltowski · Jonathan Pillow · Scott Linderman

+ Improving the Gating Mechanism of Recurrent Neural Networks

  Albert Gu · Caglar Gulcehre · Thomas Paine · Matthew Hoffman · Razvan Pascanu

+ Approximating Stacked and Bidirectional Recurrent Architectures with the Delayed Recurrent Neural Network

  Javier Turek · Shailee Jain · Vy Vo · Mihai Capotă · Alexander Huth · Theodore Willke

+ Transformation of ReLU-based recurrent neural networks from discrete-time to continuous-time

  Zahra Monfared · Daniel Durstewitz

+ Learning to Combine Top-Down and Bottom-Up Signals in Recurrent Neural Networks with Attention over Modules

  Sarthak Mittal · Alex Lamb · Anirudh Goyal · Vikram Voleti · Murray Shanahan · Guillaume Lajoie · Michael Mozer · Yoshua Bengio

+ VideoOneNet: Bidirectional Convolutional Recurrent OneNet with Trainable Data Steps for Video Processing

  Zoltán Milacski · Barnabás Póczos · Andras Lorincz

+ Frequentist Uncertainty in Recurrent Neural Networks via Blockwise Influence Functions

  Ahmed Alaa · Mihaela van der Schaar

## Interpretable

+ Multiresolution Tensor Learning for Efficient and Interpretable Spatial Analysis

  Jung Yeon Park · Kenneth Carr · Stephan Zheng · Yisong Yue · Rose Yu

+ Interpretable Off-Policy Evaluation in Reinforcement Learning by Highlighting Influential Transitions

  Omer Gottesman · Joseph Futoma · Yao Liu · Sonali Parbhoo · Leo Celi · Emma Brunskill · Finale Doshi-Velez

+ Dispersed EM-VAEs for Interpretable Text Generation

  Wenxian Shi · Hao Zhou · Ning Miao · Lei Li

+ **[已读]** Interpretable, Multidimensional, Multimodal Anomaly Detection with Negative Sampling for Detection of Device Failure

  John Sipple
  
+ Proper Network Interpretability Helps Adversarial Robustness in Classification

  Akhilan Boopathy · Sijia Liu · Gaoyuan Zhang · Cynthia Liu · Pin-Yu Chen · Shiyu Chang · Luca Daniel
  
+ Interpretations are Useful: Penalizing Explanations to Align Neural Networks with Prior Knowledge

  Laura Rieger · Chandan Singh · William Murdoch · Bin Yu
  
+ DeepCoDA: personalized interpretability for compositional health data

  Thomas Quinn · Dang Nguyen · Santu Rana · Sunil Gupta · Svetha Venkatesh
  
  
+ **[简单看了看，参考意义不大]** Interpreting Robust Optimization via Adversarial Influence Functions

  Zhun Deng · Cynthia Dwork · Jialiang Wang · Linjun Zhang
  
  主要是用adversarial influence function来理解鲁棒优化的过程的，不是广义的神经网络的。
 
+ **[简单看了看，参考意义不大]** Unsupervised Discovery of Interpretable Directions in the GAN Latent Space

  Andrey Voynov · Artem Babenko
  
  GAN的隐空间中通常包含有一些具有语义信息的方向，在这些方向上进行人类可以理解的隐空间表示的操控，可以生成受控的图像变换（在某个方向上的移动可能代表了图像的饿zooming或者recoloring，在隐空间中找到这些方向，并在这些方向上生成样本，有助于我们方便的进行样本变换。），但是目前为止，发现这些方向的过程一般都是“in a supervised manner, requiring human labels, pretrained models, or some form of self-supervision”，
  
  In this paper, we introduce an **unsupervised method to identify interpretable directions in the latent space** of a **pretrained GAN model**. By a simple **model-agnostic** procedure, we find directions corresponding to sensible semantic manipulations without any form of (self-)supervision. Furthermore, we reveal several **non-trivial findings**, which would be difficult to obtain by existing methods, e.g., a direction corresponding to background removal. (用于去除图片背景的方向)。
  
  
+ **[可以看看]** Explaining Groups of Points in Low-Dimensional Representations

  Gregory Plumb · Jonathan Terhorst · Sriram Sankararaman · Ameet Talwalkar
  
  这篇文章主要探究一个问题：在数据集样本的低维表征空间中，每一簇之间到底有什么差异？具体来讲，研究两个具体的问题：1）groupA与groupB的隐空间表征到底有什么区别？即：我们能否找到一种在显空间的变换，使得样本从groupA变换到groupB？2）对于groupA中的所有点，我们能否找出一种通用的描述，以回答“满足什么条件的样本能够算作是groupA的”，并且，这种描述在每个group上都是成立的（原话：we want to find a explanation that works for all of the points in Group A and because we want the complete set of explanations to be consistent (i.e., symmetrical and transitive) among all the groups.）。
  
+ Explainable k-Means and k-Medians Clustering

  Michal Moshkovitz · Sanjoy Dasgupta · Cyrus Rashtchian · Nave Frost
  
+ Explainable and Discourse Topic-aware Neural Language Understanding

  Yatin Chaudhary · Hinrich Schuetze · Pankaj Gupta
  
+ Problems with Shapley-value-based explanations as feature importance measures

  Indra Kumar · Suresh Venkatasubramanian · Carlos Scheidegger · Sorelle Friedler


+ The Many Shapley Values for Model Explanation
 
  Mukund Sundararajan · Amir Najmi

+ When Explanations Lie: Why Many Modified BP Attributions Fail

  Leon Sixt · Maximilian Granz · Tim Landgraf


+ **[简单看了看，参考意义不大]** Robust and Stable Black Box Explanations

  Himabindu Lakkaraju · Nino Arsov · Osbert Bastani
  
  文章本身做的是：针对数据分布偏移后的数据集，如何生成的鲁棒的可解释方法，这种偏移直觉上来讲是：在输入数据上加一个微小的扰动，可解释方法不应当因为这些扰动而有巨大的变化（然而现有的方法对这些扰动并不鲁棒）
  
  + we propose **a novel minimax objective** that can be used to construct **robust black box explanations for a given family of interpretable models**. This objective encodes the goal of returning the highest fidelity explanation with respect to the
worst-case over a set of distribution shifts.
  + we propose **a set of distribution shifts** that captures our intuition about the kinds of shifts to which interpretations should be robust. In particular, this set includes shifts that contain **perturbations to a small number of covariates**.
  + we propose algorithms for optimizing this objective in two settings: (i) explanations such as linear models with continuous parameters that can be optimized using gradient descent, in which case we use adversarial training (Goodfellow et al., 2015), and (ii) explanations such as decision sets with discrete parameters, in which case we use a sampling-based approximation in conjunction with submodular optimization (Lakkaraju et al., 2016).
  
  


+ Fairwashing explanations with off-manifold detergent

  Christopher Anders · Plamen Pasliev · Ann-Kathrin Dombrowski · Klaus-robert Mueller · Pan Kessel


+ XXAI: Extending Explainable AI Beyond Deep Models and Classifiers [workshop]

  Wojciech Samek · Andreas Holzinger · Ruth Fong · Taesup Moon · Klaus-robert Mueller
  


## Autoencoder

+ Encoding Musical Style with Transformer Autoencoders

  Kristy Choi · Curtis Hawthorne · Ian Simon · Monica Dinculescu · Jesse Engel

+ Forecasting sequential data using Consistent Koopman Autoencoders

  Omri Azencot · N. Benjamin Erichson · Vanessa Lin · Michael Mahoney

+ Latent Bernoulli Autoencoder

  Jiri Fajtl · Vasileios Argyriou · Dorothy Monekosso · Paolo Remagnino

+ Perceptual Generative Autoencoders

  Zijun Zhang · Ruixiang ZHANG · Zongpeng Li · Yoshua Bengio · Liam Paull

+ Rate-distortion optimization guided autoencoder for isometric embedding in Euclidean latent space

  Keizo Kato · Jing Zhou · Tomotake Sasaki · Akira Nakagawa

+ Educating Text Autoencoders: Latent Representation Guidance via Denoising

  Tianxiao Shen · Jonas Mueller · Regina Barzilay · Tommi Jaakkola

+ Topological Autoencoders

  Michael Moor · Max Horn · Bastian Rieck · Karsten Borgwardt

+ ControlVAE: Controllable Variational Autoencoder

  Huajie Shao · Shuochao Yao · Dachun Sun · Aston Zhang · Shengzhong Liu · Dongxin Liu · Jun Wang · Tarek Abdelzaher

+ Learning Autoencoders with Relational Regularization

  Hongteng Xu · Dixin Luo · Ricardo Henao · Svati Shah · Lawrence Carin

## LSTM

+ Do RNN and LSTM have Long Memory?

  Jingyu Zhao · Feiqing Huang · Jia Lv · Yanjie Duan · Zhen Qin · Guodong Li · Guangjian Tian
  
  
## Data augmentation

+ On the Generalization Effects of Linear Transformations in Data Augmentation
  
  Sen Wu · Hongyang Zhang · Gregory Valiant · Christopher Re
  
+ Improving Molecular Design by Stochastic Iterative Target Augmentation
  
  Kevin Yang · Wengong Jin · Kyle Swanson · Regina Barzilay · Tommi Jaakkola


+  VFlow: More Expressive Generative Flows with Variational Data Augmentation
  
  Jianfei Chen · Cheng Lu · Biqi Chenli · Jun Zhu · Tian Tian

+ **[已读]** Self-supervised Label Augmentation via Input Transformations

  Hankook Lee · Sung Ju Hwang · Jinwoo Shin
  
  一种在全监督的设定下，做数据增强的方案。所提出的方法：
  
  1. 区别于传统的self-supervised learning task（multi-task leanring: 设定两个任务，一个原始任务一个附加任务，使用两个子网络进行实现，loss函数为两个任务的和），
  本文提出的方法直接在同一个网络中同时进行原始任务+附加任务。
  
  即：对于一个图片分类任务，传统方法：子网络1用于分类，子网络2用于区分数据增强方法。本文方法：训练一个$N+M$分类的网络，前者为数据类别数，后者为增强方法数。然后对网络的输出进行融合，学习一个条件概率分布，然后将该概率的loss加入到总的loss中。
  
  
+ Distribution Augmentation for Generative Modeling
  
  Heewoo Jun · Rewon Child · Mark Chen · John Schulman · Aditya Ramesh · Alec Radford ·
Ilya Sutskever
